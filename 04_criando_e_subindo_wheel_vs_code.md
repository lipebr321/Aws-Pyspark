# Cap√≠tulo 4: Criando e Subindo Wheel no VS Code

## Introdu√ß√£o

Quando trabalhamos com aplica√ß√µes Spark mais complexas, √© comum organizar o c√≥digo em m√∫ltiplos arquivos e m√≥dulos Python. Para facilitar a distribui√ß√£o e execu√ß√£o desse c√≥digo no cluster EMR, uma pr√°tica recomendada √© empacotar tudo em um arquivo wheel (.whl). Neste cap√≠tulo, voc√™ aprender√° como estruturar seu projeto no VS Code, criar um arquivo wheel e fazer upload para o S3, preparando-o para execu√ß√£o no EMR.

## Pr√©-requisitos

Antes de come√ßar, voc√™ precisar√°:

- Visual Studio Code instalado
- Python 3.6+ instalado
- Conhecimentos b√°sicos de Python e estrutura de pacotes
- Um bucket S3 para armazenar o arquivo wheel
- AWS CLI configurada (para upload para o S3)

## Gloss√°rio para Iniciantes üìù

- **Wheel (.whl)**: Formato de pacote Python pr√©-compilado que facilita a instala√ß√£o
- **setup.py**: Arquivo que cont√©m metadados e instru√ß√µes para empacotar um projeto Python
- **VS Code**: Visual Studio Code, um editor de c√≥digo leve e poderoso
- **M√≥dulo**: Arquivo Python que cont√©m defini√ß√µes e declara√ß√µes
- **Pacote**: Diret√≥rio contendo m√≥dulos Python e um arquivo __init__.py
- **AWS CLI**: Interface de linha de comando da AWS para interagir com servi√ßos como o S3
- **S3**: Amazon Simple Storage Service, servi√ßo de armazenamento de objetos da AWS

## Estrutura de Pastas no VS Code

Uma estrutura organizada de projeto √© fundamental para facilitar o desenvolvimento e a manuten√ß√£o. Vamos criar uma estrutura t√≠pica para um projeto Spark:

```
meu_projeto_spark/
‚îÇ
‚îú‚îÄ‚îÄ setup.py                  # Arquivo de configura√ß√£o para criar o wheel
‚îú‚îÄ‚îÄ README.md                 # Documenta√ß√£o do projeto
‚îú‚îÄ‚îÄ requirements.txt          # Depend√™ncias do projeto
‚îÇ
‚îú‚îÄ‚îÄ src/                      # C√≥digo-fonte do projeto
‚îÇ   ‚îú‚îÄ‚îÄ meu_pacote/           # Pacote principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py       # Torna o diret√≥rio um pacote Python
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformacoes.py # M√≥dulo com transforma√ß√µes de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validacoes.py     # M√≥dulo com fun√ß√µes de valida√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py          # M√≥dulo com fun√ß√µes utilit√°rias
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ scripts/              # Scripts execut√°veis
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ processar_dados.py # Script principal para spark-submit
‚îÇ
‚îî‚îÄ‚îÄ tests/                    # Testes unit√°rios
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_transformacoes.py
    ‚îî‚îÄ‚îÄ test_validacoes.py
```

üñºÔ∏è **Print da estrutura de pastas no VS Code**
‚û°Ô∏è Abra o VS Code e crie a estrutura de pastas conforme mostrado acima.

### Criando a Estrutura de Pastas

Voc√™ pode criar esta estrutura manualmente no VS Code ou usar comandos no terminal:

```bash
# Criar a estrutura de diret√≥rios
mkdir -p meu_projeto_spark/src/meu_pacote
mkdir -p meu_projeto_spark/src/scripts
mkdir -p meu_projeto_spark/tests

# Criar arquivos __init__.py vazios
touch meu_projeto_spark/src/meu_pacote/__init__.py
touch meu_projeto_spark/src/scripts/__init__.py
touch meu_projeto_spark/tests/__init__.py

# Criar arquivos b√°sicos
touch meu_projeto_spark/setup.py
touch meu_projeto_spark/README.md
touch meu_projeto_spark/requirements.txt
```

## Criando os Arquivos do Projeto

Vamos criar os arquivos principais do projeto:

### 1. setup.py

O arquivo `setup.py` √© essencial para criar o pacote wheel. Ele cont√©m metadados sobre o projeto e instru√ß√µes para o empacotamento:

```python
# setup.py
from setuptools import setup, find_packages

setup(
    name="meu_pacote_spark",
    version="0.1.0",
    author="Seu Nome",
    author_email="seu.email@exemplo.com",
    description="Pacote para processamento de dados com Spark",
    long_description=open("README.md").read(),
    long_description_content_type="text/markdown",
    url="https://github.com/seu-usuario/meu-projeto-spark",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.6",
    install_requires=[
        "pyspark>=3.0.0",
        "pandas>=1.0.0",
    ],
)
```

üñºÔ∏è **Print do arquivo setup.py no VS Code**
‚û°Ô∏è Crie o arquivo setup.py com o conte√∫do acima.

### 2. README.md

```markdown
# Meu Pacote Spark

Pacote Python para processamento de dados com Apache Spark no EMR.

## Instala√ß√£o

```bash
pip install meu_pacote_spark-0.1.0-py3-none-any.whl
```

## Uso

```python
from meu_pacote.transformacoes import transformar_dados
from meu_pacote.validacoes import validar_schema

# Seu c√≥digo aqui
```
```

### 3. requirements.txt

```
pyspark>=3.0.0
pandas>=1.0.0
pytest>=6.0.0
```

### 4. src/meu_pacote/transformacoes.py

```python
# src/meu_pacote/transformacoes.py
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, upper, to_date, year, month, dayofmonth

def transformar_colunas_texto(df: DataFrame, colunas: list) -> DataFrame:
    """
    Converte colunas de texto para mai√∫sculas.
    
    Args:
        df: DataFrame do Spark
        colunas: Lista de nomes de colunas para transformar
        
    Returns:
        DataFrame com as colunas transformadas
    """
    for coluna in colunas:
        df = df.withColumn(coluna, upper(col(coluna)))
    return df

def extrair_componentes_data(df: DataFrame, coluna_data: str) -> DataFrame:
    """
    Extrai ano, m√™s e dia de uma coluna de data.
    
    Args:
        df: DataFrame do Spark
        coluna_data: Nome da coluna de data
        
    Returns:
        DataFrame com colunas adicionais para ano, m√™s e dia
    """
    return df.withColumn("data_formatada", to_date(col(coluna_data))) \
             .withColumn("ano", year(col("data_formatada"))) \
             .withColumn("mes", month(col("data_formatada"))) \
             .withColumn("dia", dayofmonth(col("data_formatada")))

def calcular_metricas(df: DataFrame, coluna_valor: str) -> DataFrame:
    """
    Calcula m√©tricas adicionais com base em uma coluna de valor.
    
    Args:
        df: DataFrame do Spark
        coluna_valor: Nome da coluna com valores num√©ricos
        
    Returns:
        DataFrame com colunas adicionais de m√©tricas
    """
    return df.withColumn("valor_com_imposto", col(coluna_valor) * 1.1) \
             .withColumn("valor_com_desconto", col(coluna_valor) * 0.9) \
             .withColumn("valor_arredondado", col(coluna_valor).cast("integer"))
```

### 5. src/meu_pacote/validacoes.py

```python
# src/meu_pacote/validacoes.py
from pyspark.sql import DataFrame
from pyspark.sql.types import StructType

def validar_schema(df: DataFrame, schema_esperado: StructType) -> bool:
    """
    Valida se o DataFrame possui o schema esperado.
    
    Args:
        df: DataFrame do Spark
        schema_esperado: Schema esperado
        
    Returns:
        True se o schema for v√°lido, False caso contr√°rio
    """
    return df.schema == schema_esperado

def validar_valores_nulos(df: DataFrame, colunas: list) -> dict:
    """
    Verifica se existem valores nulos nas colunas especificadas.
    
    Args:
        df: DataFrame do Spark
        colunas: Lista de nomes de colunas para verificar
        
    Returns:
        Dicion√°rio com contagem de nulos por coluna
    """
    resultado = {}
    for coluna in colunas:
        contagem = df.filter(df[coluna].isNull()).count()
        resultado[coluna] = contagem
    return resultado

def validar_intervalo_valores(df: DataFrame, coluna: str, min_valor: float, max_valor: float) -> bool:
    """
    Verifica se todos os valores de uma coluna est√£o dentro de um intervalo.
    
    Args:
        df: DataFrame do Spark
        coluna: Nome da coluna para verificar
        min_valor: Valor m√≠nimo aceit√°vel
        max_valor: Valor m√°ximo aceit√°vel
        
    Returns:
        True se todos os valores estiverem no intervalo, False caso contr√°rio
    """
    fora_intervalo = df.filter((col(coluna) < min_valor) | (col(coluna) > max_valor)).count()
    return fora_intervalo == 0
```

### 6. src/meu_pacote/utils.py

```python
# src/meu_pacote/utils.py
from pyspark.sql import SparkSession, DataFrame
import logging

def criar_spark_session(app_name: str) -> SparkSession:
    """
    Cria uma sess√£o Spark com configura√ß√µes otimizadas.
    
    Args:
        app_name: Nome da aplica√ß√£o
        
    Returns:
        Sess√£o Spark configurada
    """
    return SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.sql.shuffle.partitions", "200") \
        .getOrCreate()

def ler_dados_csv(spark: SparkSession, caminho: str, tem_cabecalho: bool = True) -> DataFrame:
    """
    L√™ dados de um arquivo CSV.
    
    Args:
        spark: Sess√£o Spark
        caminho: Caminho para o arquivo CSV
        tem_cabecalho: Indica se o CSV tem cabe√ßalho
        
    Returns:
        DataFrame com os dados do CSV
    """
    return spark.read \
        .option("header", str(tem_cabecalho).lower()) \
        .option("inferSchema", "true") \
        .csv(caminho)

def salvar_como_parquet(df: DataFrame, caminho: str, modo: str = "overwrite", particionar_por: list = None) -> None:
    """
    Salva um DataFrame como arquivo Parquet.
    
    Args:
        df: DataFrame do Spark
        caminho: Caminho para salvar o arquivo
        modo: Modo de escrita (overwrite, append, etc.)
        particionar_por: Lista de colunas para particionar os dados
    """
    writer = df.write.mode(modo)
    
    if particionar_por:
        writer = writer.partitionBy(*particionar_por)
    
    writer.parquet(caminho)
    logging.info(f"Dados salvos com sucesso em: {caminho}")
```

### 7. src/scripts/processar_dados.py

```python
# src/scripts/processar_dados.py
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType
import argparse
import logging
import sys

# Importar m√≥dulos do nosso pacote
from meu_pacote.utils import criar_spark_session, ler_dados_csv, salvar_como_parquet
from meu_pacote.transformacoes import transformar_colunas_texto, extrair_componentes_data, calcular_metricas
from meu_pacote.validacoes import validar_schema, validar_valores_nulos

def configurar_logging():
    """Configura o sistema de logging."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )

def definir_schema_vendas():
    """Define o schema esperado para os dados de vendas."""
    return StructType([
        StructField("id_venda", StringType(), False),
        StructField("produto", StringType(), True),
        StructField("categoria", StringType(), True),
        StructField("valor", DoubleType(), True),
        StructField("data_venda", StringType(), True),
        StructField("cliente", StringType(), True)
    ])

def processar_dados(input_path, output_path):
    """
    Fun√ß√£o principal para processar os dados.
    
    Args:
        input_path: Caminho para os dados de entrada no S3
        output_path: Caminho para salvar os resultados no S3
    """
    logging.info(f"Iniciando processamento de dados")
    logging.info(f"Entrada: {input_path}")
    logging.info(f"Sa√≠da: {output_path}")
    
    # Criar sess√£o Spark
    spark = criar_spark_session("Processamento de Vendas")
    
    try:
        # Ler dados
        df = ler_dados_csv(spark, input_path)
        logging.info(f"Dados carregados com sucesso. Total de registros: {df.count()}")
        
        # Validar schema
        schema_esperado = definir_schema_vendas()
        if not validar_schema(df, schema_esperado):
            logging.warning("O schema dos dados n√£o corresponde ao esperado!")
            logging.info("Schema atual:")
            df.printSchema()
        
        # Verificar valores nulos
        colunas_importantes = ["produto", "valor", "data_venda"]
        nulos = validar_valores_nulos(df, colunas_importantes)
        for coluna, contagem in nulos.items():
            if contagem > 0:
                logging.warning(f"Coluna {coluna} cont√©m {contagem} valores nulos")
        
        # Aplicar transforma√ß√µes
        df_transformado = df
        
        # Transformar texto para mai√∫sculas
        df_transformado = transformar_colunas_texto(df_transformado, ["produto", "categoria", "cliente"])
        
        # Extrair componentes de data
        df_transformado = extrair_componentes_data(df_transformado, "data_venda")
        
        # Calcular m√©tricas adicionais
        df_transformado = calcular_metricas(df_transformado, "valor")
        
        # Mostrar amostra dos dados processados
        logging.info("Amostra dos dados processados:")
        df_transformado.show(5, truncate=False)
        
        # Salvar resultados
        salvar_como_parquet(
            df_transformado, 
            output_path, 
            particionar_por=["ano", "mes"]
        )
        
        logging.info(f"Processamento conclu√≠do com sucesso!")
        
    except Exception as e:
        logging.error(f"Erro durante o processamento: {str(e)}")
        raise
    finally:
        spark.stop()

def main():
    """Fun√ß√£o principal que processa argumentos da linha de comando."""
    parser = argparse.ArgumentParser(description='Processador de dados de vendas')
    parser.add_argument('--input', required=True, help='Caminho de entrada dos dados (S3)')
    parser.add_argument('--output', required=True, help='Caminho de sa√≠da para os resultados (S3)')
    
    args = parser.parse_args()
    
    configurar_logging()
    processar_dados(args.input, args.output)

if __name__ == "__main__":
    main()
```

## Criando o Arquivo Wheel

Agora que temos a estrutura do projeto e os arquivos necess√°rios, vamos criar o arquivo wheel:

### Passo 1: Instalar as Ferramentas Necess√°rias

Primeiro, precisamos instalar as ferramentas para criar o wheel:

```bash
pip install wheel setuptools
```

### Passo 2: Executar o Comando para Criar o Wheel

Navegue at√© o diret√≥rio raiz do projeto e execute:

```bash
python setup.py bdist_wheel
```

üñºÔ∏è **Print do terminal executando o comando python setup.py bdist_wheel**
‚û°Ô∏è Execute o comando no terminal integrado do VS Code e observe a sa√≠da.

Este comando criar√° um diret√≥rio `dist` contendo o arquivo wheel, com um nome similar a:

```
dist/meu_pacote_spark-0.1.0-py3-none-any.whl
```

üí° **Dica**: O nome do arquivo wheel segue o formato `{nome_pacote}-{vers√£o}-{python_tag}-{abi_tag}-{plataforma_tag}.whl`.

## Subindo o Wheel para o S3

Agora que temos o arquivo wheel, vamos fazer upload para o S3 para que possamos us√°-lo no EMR:

### Passo 1: Verificar o Arquivo Wheel Gerado

```bash
ls -la dist/
```

### Passo 2: Fazer Upload para o S3 usando AWS CLI

```bash
aws s3 cp dist/meu_pacote_spark-0.1.0-py3-none-any.whl s3://meu-bucket/wheels/
```

üñºÔ∏è **Print do terminal executando o comando aws s3 cp**
‚û°Ô∏è Execute o comando aws s3 cp e observe a sa√≠da confirmando o upload.

üí° **Dica**: Certifique-se de que o usu√°rio que est√° executando o comando tem permiss√µes para escrever no bucket S3.

### Passo 3: Verificar se o Upload foi Bem-sucedido

```bash
aws s3 ls s3://meu-bucket/wheels/
```

## Boas Pr√°ticas para Organiza√ß√£o de C√≥digo üî•

1. **Estrutura Modular**: Divida seu c√≥digo em m√≥dulos com responsabilidades bem definidas.

2. **Documenta√ß√£o**: Adicione docstrings a todas as fun√ß√µes e classes para facilitar o entendimento.

3. **Tipagem**: Use type hints para melhorar a legibilidade e permitir verifica√ß√£o est√°tica de tipos.

4. **Testes**: Inclua testes unit√°rios para validar a funcionalidade do c√≥digo.

5. **Logging**: Implemente logging adequado para facilitar a depura√ß√£o.

6. **Tratamento de Erros**: Use blocos try/except para lidar com exce√ß√µes de forma elegante.

7. **Configura√ß√£o Centralizada**: Mantenha configura√ß√µes em um local centralizado para f√°cil manuten√ß√£o.

8. **Versionamento**: Use controle de vers√£o (Git) e siga o versionamento sem√¢ntico para seus pacotes.

## Dicas para Desenvolvimento no VS Code üí°

1. **Extens√µes √öteis**:
   - Python (Microsoft)
   - Pylance (Microsoft)
   - Python Docstring Generator
   - GitLens
   - Remote - SSH (para desenvolvimento remoto)

2. **Configura√ß√£o do Linter**:
   Crie um arquivo `.vscode/settings.json` com:
   ```json
   {
     "python.linting.enabled": true,
     "python.linting.pylintEnabled": true,
     "python.linting.flake8Enabled": true,
     "python.formatting.provider": "black"
   }
   ```

3. **Depura√ß√£o**:
   Configure o arquivo `.vscode/launch.json` para depurar seu c√≥digo:
   ```json
   {
     "version": "0.2.0",
     "configurations": [
       {
         "name": "Python: Current File",
         "type": "python",
         "request": "launch",
         "program": "${file}",
         "console": "integratedTerminal"
       }
     ]
   }
   ```

4. **Terminal Integrado**: Use o terminal integrado do VS Code para executar comandos sem sair do editor.

## Solu√ß√£o de Problemas Comuns

### Erro "No module named 'setuptools'"

Solu√ß√£o:
```bash
pip install setuptools
```

### Erro ao Criar o Wheel

Verifique:
- Se o arquivo setup.py est√° correto
- Se todos os arquivos referenciados existem
- Se as depend√™ncias est√£o instaladas

### Erro ao Fazer Upload para o S3

Verifique:
- Suas credenciais AWS
- Permiss√µes do bucket
- Conex√£o com a internet

## Conclus√£o

Neste cap√≠tulo, voc√™ aprendeu como estruturar um projeto Python no VS Code, criar um arquivo wheel e fazer upload para o S3. Esta abordagem facilita a organiza√ß√£o, manuten√ß√£o e distribui√ß√£o do seu c√≥digo para execu√ß√£o no EMR. No pr√≥ximo cap√≠tulo, aprenderemos como executar este wheel diretamente no cluster EMR usando o spark-submit.

---

üîç **Pr√≥ximo Cap√≠tulo**: [Rodando Spark com Wheel no S3](05_rodando_spark_com_wheel_no_s3.md)
