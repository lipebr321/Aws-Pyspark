# Cap√≠tulo 7: Rodando com Arquivo JAR

## Introdu√ß√£o

Al√©m de executar c√≥digo Python no Spark, muitas vezes precisamos trabalhar com aplica√ß√µes escritas em Java ou Scala, que s√£o compiladas em arquivos JAR. O Apache Spark foi originalmente desenvolvido em Scala e oferece suporte nativo para essas linguagens. Neste cap√≠tulo, voc√™ aprender√° como compilar um arquivo JAR para o Spark e execut√°-lo no cluster EMR, al√©m de entender as diferen√ßas entre aplica√ß√µes Python e JAR no contexto do Spark.

## Pr√©-requisitos

Antes de come√ßar, voc√™ precisar√°:

- Um cluster EMR em execu√ß√£o
- Conhecimentos b√°sicos de Java ou Scala
- JDK (Java Development Kit) instalado
- Maven ou SBT para compila√ß√£o
- Acesso SSH ao n√≥ master do cluster

## Gloss√°rio para Iniciantes üìù

- **JAR**: Java ARchive, formato de pacote que cont√©m classes Java compiladas e recursos
- **Scala**: Linguagem de programa√ß√£o que combina programa√ß√£o orientada a objetos e funcional
- **Maven**: Ferramenta de automa√ß√£o de compila√ß√£o para projetos Java
- **SBT**: Scala Build Tool, ferramenta de compila√ß√£o para projetos Scala
- **JVM**: Java Virtual Machine, ambiente de execu√ß√£o para bytecode Java
- **Bytecode**: C√≥digo intermedi√°rio gerado pela compila√ß√£o de c√≥digo Java ou Scala
- **Classe Principal**: Ponto de entrada de uma aplica√ß√£o Java/Scala (cont√©m m√©todo main)

## Como Compilar um .jar em Java/Scala para o Spark

### Estrutura de um Projeto Scala para Spark

Vamos come√ßar com a estrutura b√°sica de um projeto Scala para Spark usando SBT:

```
meu-projeto-spark/
‚îú‚îÄ‚îÄ build.sbt                 # Arquivo de configura√ß√£o do SBT
‚îú‚îÄ‚îÄ project/
‚îÇ   ‚îî‚îÄ‚îÄ build.properties      # Vers√£o do SBT
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ main/
        ‚îî‚îÄ‚îÄ scala/
            ‚îî‚îÄ‚îÄ com/
                ‚îî‚îÄ‚îÄ exemplo/
                    ‚îî‚îÄ‚îÄ SparkApp.scala  # C√≥digo-fonte Scala
```

### Arquivo build.sbt

O arquivo `build.sbt` define as depend√™ncias e configura√ß√µes do projeto:

```scala
name := "MeuProjetoSpark"
version := "1.0"
scalaVersion := "2.12.15"

// Depend√™ncias
libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "3.3.1" % "provided",
  "org.apache.spark" %% "spark-sql" % "3.3.1" % "provided"
)

// Configura√ß√µes para criar um "fat JAR" (com todas as depend√™ncias)
assembly / assemblyJarName := "meu-projeto-spark-assembly-1.0.jar"

// Resolver conflitos de arquivos durante o assembly
assembly / assemblyMergeStrategy := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case "application.conf" => MergeStrategy.concat
  case x => MergeStrategy.first
}
```
‚û°Ô∏è Crie o arquivo build.sbt com o conte√∫do acima.

üí° **Dica**: O modificador `% "provided"` indica que as depend√™ncias do Spark n√£o ser√£o inclu√≠das no JAR final, pois elas j√° est√£o dispon√≠veis no ambiente de execu√ß√£o do EMR.

### Arquivo project/build.properties

```
sbt.version=1.8.0
```

### Exemplo de C√≥digo Scala

Vamos criar um exemplo simples de aplica√ß√£o Spark em Scala:

```scala
// src/main/scala/com/exemplo/SparkApp.scala
package com.exemplo

import org.apache.spark.sql.{SparkSession, DataFrame}
import org.apache.spark.sql.functions._

object SparkApp {
  def main(args: Array[String]): Unit = {
    if (args.length < 2) {
      System.err.println("Uso: SparkApp <input-path> <output-path>")
      System.exit(1)
    }
    
    val inputPath = args(0)
    val outputPath = args(1)
    
    // Criar sess√£o Spark
    val spark = SparkSession.builder()
      .appName("Exemplo Spark em Scala")
      .getOrCreate()
    
    try {
      println(s"Processando dados de $inputPath para $outputPath")
      
      // Ler dados
      val df = spark.read
        .option("header", "true")
        .option("inferSchema", "true")
        .csv(inputPath)
      
      println(s"Schema dos dados:")
      df.printSchema()
      
      // Realizar transforma√ß√µes
      val resultDF = processarDados(df)
      
      // Mostrar amostra dos resultados
      println("Amostra dos dados processados:")
      resultDF.show(5)
      
      // Salvar resultados
      resultDF.write
        .mode("overwrite")
        .parquet(outputPath)
      
      println(s"Processamento conclu√≠do com sucesso. Resultados salvos em $outputPath")
    } finally {
      spark.stop()
    }
  }
  
  def processarDados(df: DataFrame): DataFrame = {
    // Exemplo de transforma√ß√µes
    df.withColumn("valor_com_imposto", col("valor") * 1.1)
      .withColumn("ano", year(col("data")))
      .withColumn("mes", month(col("data")))
      .withColumn("categoria_upper", upper(col("categoria")))
  }
}
```


‚û°Ô∏è Crie o arquivo SparkApp.scala com o conte√∫do acima.

### Compilando o Projeto com SBT

Para compilar o projeto e criar o arquivo JAR:

```bash
# Navegar at√© o diret√≥rio do projeto
cd meu-projeto-spark

# Compilar e criar o JAR
sbt clean assembly
```


‚û°Ô∏è Execute o comando e observe a sa√≠da mostrando o processo de compila√ß√£o.

O arquivo JAR ser√° gerado em `target/scala-2.12/meu-projeto-spark-assembly-1.0.jar`.

### Compilando o Projeto com Maven

Alternativamente, voc√™ pode usar Maven para projetos Java. Aqui est√° um exemplo de `pom.xml`:

```xml
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.exemplo</groupId>
    <artifactId>meu-projeto-spark</artifactId>
    <version>1.0</version>

    <properties>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <encoding>UTF-8</encoding>
        <scala.version>2.12.15</scala.version>
        <scala.compat.version>2.12</scala.compat.version>
        <spark.version>3.3.1</spark.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.compat.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.compat.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>4.5.6</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>com.exemplo.SparkApp</mainClass>
                                </transformer>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```

Para compilar com Maven:

```bash
mvn clean package
```

O arquivo JAR ser√° gerado em `target/meu-projeto-spark-1.0.jar`.

## Exemplo de Spark-Submit com .jar

Ap√≥s compilar o JAR, voc√™ precisa fazer upload para o S3 e execut√°-lo no cluster EMR:

### Passo 1: Fazer Upload do JAR para o S3

```bash
aws s3 cp target/scala-2.12/meu-projeto-spark-assembly-1.0.jar s3://meu-bucket/jars/
```


‚û°Ô∏è Execute o comando aws s3 cp e observe a sa√≠da confirmando o upload.

### Passo 2: Executar o JAR com Spark-Submit

```bash
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --class com.exemplo.SparkApp \
  s3://meu-bucket/jars/meu-projeto-spark-assembly-1.0.jar \
  s3://meu-bucket/dados/vendas.csv \
  s3://meu-bucket/resultados/vendas_processadas_scala
```


‚û°Ô∏è Execute o comando spark-submit conforme mostrado acima, substituindo os caminhos do S3 pelos seus.

### Par√¢metros Espec√≠ficos para JARs

- `--class`: Especifica a classe principal (com o m√©todo main) que ser√° executada
- `--jars`: Lista de JARs adicionais necess√°rios (separados por v√≠rgula)
- `--packages`: Depend√™ncias Maven a serem baixadas automaticamente

### Exemplo Avan√ßado com Configura√ß√µes Adicionais

```bash
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 4g \
  --executor-cores 2 \
  --num-executors 10 \
  --conf spark.yarn.submit.waitAppCompletion=true \
  --conf spark.sql.shuffle.partitions=200 \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --class com.exemplo.SparkApp \
  --jars s3://meu-bucket/jars/biblioteca-auxiliar.jar \
  s3://meu-bucket/jars/meu-projeto-spark-assembly-1.0.jar \
  s3://meu-bucket/dados/vendas.csv \
  s3://meu-bucket/resultados/vendas_processadas_scala
```

## Diferen√ßa entre .py e .jar no Contexto do Spark

### Arquitetura de Execu√ß√£o

#### Python (.py)

1. **Processo de Execu√ß√£o**:
   - O c√≥digo Python √© interpretado pelo PySpark
   - PySpark se comunica com o Spark Core (JVM) atrav√©s do Py4J
   - H√° uma camada adicional de comunica√ß√£o entre Python e JVM

2. **Desempenho**:
   - Geralmente mais lento devido √† serializa√ß√£o/desserializa√ß√£o entre Python e JVM
   - Overhead de comunica√ß√£o entre processos

3. **Uso de Mem√≥ria**:
   - Maior consumo de mem√≥ria devido √† duplica√ß√£o de dados entre JVM e Python


‚û°Ô∏è O PySpark utiliza Py4J para comunica√ß√£o entre o c√≥digo Python e o Spark Core na JVM.

#### Java/Scala (.jar)

1. **Processo de Execu√ß√£o**:
   - O c√≥digo Java/Scala √© executado diretamente na JVM
   - Comunica√ß√£o direta com o Spark Core
   - Sem camadas adicionais de comunica√ß√£o

2. **Desempenho**:
   - Geralmente mais r√°pido, especialmente para opera√ß√µes complexas
   - Sem overhead de serializa√ß√£o/desserializa√ß√£o entre linguagens

3. **Uso de Mem√≥ria**:
   - Mais eficiente, sem duplica√ß√£o de dados entre ambientes


‚û°Ô∏è Aplica√ß√µes Java/Scala s√£o executadas diretamente na JVM, sem camadas intermedi√°rias.

### Quando Usar Cada Um

#### Python (.py)

**Vantagens**:
- Desenvolvimento mais r√°pido
- Sintaxe mais simples e concisa
- Ecossistema rico para ci√™ncia de dados (pandas, numpy, scikit-learn)
- Melhor para prototipagem e explora√ß√£o de dados

**Ideal para**:
- Cientistas de dados e analistas
- Projetos que requerem integra√ß√£o com bibliotecas Python
- Prototipagem r√°pida e an√°lise explorat√≥ria
- Equipes com conhecimento predominante em Python

#### Java/Scala (.jar)

**Vantagens**:
- Melhor desempenho para opera√ß√µes complexas
- Tipagem est√°tica (menos erros em tempo de execu√ß√£o)
- Acesso direto a todas as funcionalidades do Spark
- Melhor para aplica√ß√µes de produ√ß√£o de alto desempenho

**Ideal para**:
- Engenheiros de dados e desenvolvedores
- Aplica√ß√µes de produ√ß√£o com requisitos de desempenho
- Processamento de grandes volumes de dados
- Equipes com conhecimento em Java/Scala

### Compara√ß√£o de C√≥digo

Vamos comparar o mesmo processamento em Python e Scala:

#### Python (PySpark)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, upper, year, month

# Criar sess√£o Spark
spark = SparkSession.builder \
    .appName("Exemplo PySpark") \
    .getOrCreate()

# Ler dados
df = spark.read.option("header", "true") \
               .option("inferSchema", "true") \
               .csv("s3://meu-bucket/dados/vendas.csv")

# Transformar dados
df_processado = df.withColumn("valor_com_imposto", col("valor") * 1.1) \
                  .withColumn("ano", year(col("data"))) \
                  .withColumn("mes", month(col("data"))) \
                  .withColumn("categoria_upper", upper(col("categoria")))

# Salvar resultados
df_processado.write.mode("overwrite") \
                   .parquet("s3://meu-bucket/resultados/vendas_processadas")

# Encerrar sess√£o
spark.stop()
```

#### Scala

```scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

// Criar sess√£o Spark
val spark = SparkSession.builder()
  .appName("Exemplo Scala")
  .getOrCreate()

// Ler dados
val df = spark.read
  .option("header", "true")
  .option("inferSchema", "true")
  .csv("s3://meu-bucket/dados/vendas.csv")

// Transformar dados
val dfProcessado = df.withColumn("valor_com_imposto", col("valor") * 1.1)
  .withColumn("ano", year(col("data")))
  .withColumn("mes", month(col("data")))
  .withColumn("categoria_upper", upper(col("categoria")))

// Salvar resultados
dfProcessado.write
  .mode("overwrite")
  .parquet("s3://meu-bucket/resultados/vendas_processadas")

// Encerrar sess√£o
spark.stop()
```

Como voc√™ pode ver, a l√≥gica e a estrutura s√£o muito semelhantes, com diferen√ßas principalmente na sintaxe.

## Boas Pr√°ticas para Desenvolvimento com Java/Scala üî•

1. **Gerenciamento de Depend√™ncias**: Use Maven ou SBT para gerenciar depend√™ncias de forma consistente.

2. **Serializa√ß√£o**: Implemente `Serializable` em classes personalizadas para evitar erros de serializa√ß√£o.

3. **Kryo Serializer**: Use o Kryo Serializer para melhor desempenho:
   ```bash
   --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
   ```

4. **Tipagem**: Aproveite o sistema de tipos est√°ticos para detectar erros em tempo de compila√ß√£o.

5. **Datasets vs DataFrames**: Use Datasets para opera√ß√µes que se beneficiam de tipagem est√°tica.

6. **Logging**: Implemente logging adequado para facilitar a depura√ß√£o:
   ```scala
   import org.apache.log4j.{Logger, Level}
   val logger = Logger.getLogger(getClass.getName)
   logger.info("Processando dados...")
   ```

7. **Testes Unit√°rios**: Escreva testes unit√°rios com frameworks como ScalaTest ou JUnit.

8. **Documenta√ß√£o**: Use Scaladoc/Javadoc para documentar seu c√≥digo.

## Solu√ß√£o de Problemas Comuns

### Erro "ClassNotFoundException"

Verifique:
- Se o par√¢metro `--class` est√° correto e corresponde ao nome completo da classe (incluindo o pacote)
- Se o JAR cont√©m a classe especificada (use `jar tvf meu-jar.jar | grep ClasseName`)
- Se h√° conflitos de vers√£o entre as depend√™ncias

### Erro "NoSuchMethodError"

Poss√≠veis causas:
- Incompatibilidade de vers√µes entre bibliotecas
- M√©todo n√£o encontrado na vers√£o da biblioteca dispon√≠vel no cluster

Solu√ß√£o:
- Verifique as vers√µes das depend√™ncias
- Use `--packages` para garantir a vers√£o correta das depend√™ncias

### Erro "OutOfMemoryError"

Poss√≠veis solu√ß√µes:
- Aumente a mem√≥ria do driver e executores
- Otimize as transforma√ß√µes para reduzir o uso de mem√≥ria
- Ajuste as configura√ß√µes de garbage collection

## Conclus√£o

Neste cap√≠tulo, voc√™ aprendeu como compilar um arquivo JAR para o Spark usando Java ou Scala, como execut√°-lo no cluster EMR e as diferen√ßas entre aplica√ß√µes Python e JAR no contexto do Spark. Cada abordagem tem suas vantagens e desvantagens, e a escolha entre elas depende dos requisitos do projeto, das habilidades da equipe e dos objetivos de desempenho. No pr√≥ximo cap√≠tulo, aprenderemos como acessar e analisar logs para depurar aplica√ß√µes Spark no EMR.

---

üîç **Pr√≥ximo Cap√≠tulo**: [Acessando Logs e Debugando](08_acessando_logs_e_debugando.md)
