# Cap√≠tulo 9: Verificando Resultado no S3

## Introdu√ß√£o

Ap√≥s executar suas aplica√ß√µes Spark no EMR e processar os dados, √© fundamental verificar os resultados gerados no S3. A valida√ß√£o dos dados processados √© uma etapa cr√≠tica para garantir a qualidade e a integridade dos resultados. Neste cap√≠tulo, voc√™ aprender√° como acessar e verificar arquivos Parquet e outros formatos no S3, navegar pelo console da AWS e utilizar ferramentas como o Amazon Athena para consultar e validar os dados processados.

## Pr√©-requisitos

Antes de come√ßar, voc√™ precisar√°:

- Uma conta AWS ativa
- Dados processados armazenados no S3
- Permiss√µes adequadas para acessar o S3
- Conhecimentos b√°sicos de SQL (para uso do Athena)

## Gloss√°rio para Iniciantes üìù

- **S3**: Amazon Simple Storage Service, servi√ßo de armazenamento de objetos da AWS
- **Bucket**: Container principal para armazenamento de objetos no S3
- **Parquet**: Formato colunar de armazenamento otimizado para an√°lise de dados
- **Parti√ß√£o**: Divis√£o dos dados em diret√≥rios baseados em valores de colunas espec√≠ficas
- **Athena**: Servi√ßo de consulta interativa da AWS que permite analisar dados no S3 usando SQL
- **Glue**: Servi√ßo de cataloga√ß√£o e ETL da AWS que integra com S3 e Athena
- **Prefixo**: Caminho dentro de um bucket S3, similar a um diret√≥rio em sistemas de arquivos

## Como Acessar Arquivos Parquet Gerados

Os arquivos Parquet s√£o um formato colunar otimizado para an√°lise de dados, amplamente utilizado em aplica√ß√µes Spark. Vamos explorar diferentes maneiras de acessar e verificar esses arquivos.

### Usando o Console S3 da AWS

#### Passo 1: Acessar o Console S3

1. Fa√ßa login no [Console AWS](https://console.aws.amazon.com/)
2. Navegue at√© o servi√ßo S3


‚û°Ô∏è Na barra de pesquisa superior, digite "S3" e selecione o servi√ßo "S3" nos resultados.

#### Passo 2: Navegar at√© o Bucket

1. Na lista de buckets, clique no bucket onde seus dados foram salvos
2. Navegue pelos prefixos (diret√≥rios) at√© encontrar seus dados processados


‚û°Ô∏è Clique no nome do bucket que cont√©m seus dados processados.

#### Passo 3: Explorar a Estrutura de Particionamento

Se seus dados foram particionados (por exemplo, por ano e m√™s), voc√™ ver√° uma estrutura de diret√≥rios:

```
meu-bucket/
‚îî‚îÄ‚îÄ resultados/
    ‚îî‚îÄ‚îÄ vendas_processadas/
        ‚îú‚îÄ‚îÄ ano=2022/
        ‚îÇ   ‚îú‚îÄ‚îÄ mes=1/
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ part-00000-xxxx.parquet
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ part-00001-xxxx.parquet
        ‚îÇ   ‚îî‚îÄ‚îÄ mes=2/
        ‚îÇ       ‚îú‚îÄ‚îÄ part-00000-xxxx.parquet
        ‚îÇ       ‚îî‚îÄ‚îÄ part-00001-xxxx.parquet
        ‚îî‚îÄ‚îÄ ano=2023/
            ‚îî‚îÄ‚îÄ mes=1/
                ‚îú‚îÄ‚îÄ part-00000-xxxx.parquet
                ‚îî‚îÄ‚îÄ part-00001-xxxx.parquet
```


‚û°Ô∏è Navegue pelos diret√≥rios particionados para visualizar a estrutura.

#### Passo 4: Verificar Metadados dos Arquivos

1. Clique em um arquivo Parquet
2. Na aba "Vis√£o geral", voc√™ ver√° metadados como tamanho, data de modifica√ß√£o, classe de armazenamento, etc.

‚û°Ô∏è Clique em um arquivo Parquet e observe os metadados dispon√≠veis.

### Usando a AWS CLI

A AWS CLI oferece uma maneira eficiente de interagir com o S3 via linha de comando:

#### Listar Objetos no S3

```bash
# Listar o conte√∫do de um prefixo
aws s3 ls s3://meu-bucket/resultados/vendas_processadas/

# Listar recursivamente (incluindo subdiret√≥rios)
aws s3 ls s3://meu-bucket/resultados/vendas_processadas/ --recursive

# Listar apenas arquivos Parquet
aws s3 ls s3://meu-bucket/resultados/vendas_processadas/ --recursive | grep .parquet
```


‚û°Ô∏è Execute o comando aws s3 ls e observe a lista de objetos no S3.

#### Verificar Tamanho Total dos Dados

```bash
# Calcular o tamanho total dos dados
aws s3 ls s3://meu-bucket/resultados/vendas_processadas/ --recursive --summarize
```

Exemplo de sa√≠da:
```
Total Objects: 42
Total Size: 1.2 GB
```

#### Baixar Arquivos para Inspe√ß√£o Local

```bash
# Baixar um arquivo espec√≠fico
aws s3 cp s3://meu-bucket/resultados/vendas_processadas/ano=2023/mes=1/part-00000-xxxx.parquet ./

# Baixar um diret√≥rio inteiro
aws s3 cp s3://meu-bucket/resultados/vendas_processadas/ano=2023/mes=1/ ./ --recursive
```

### Usando PySpark para Verificar os Dados

PySpark √© uma excelente ferramenta para verificar e validar os dados Parquet:

```python
from pyspark.sql import SparkSession

# Inicializar a sess√£o Spark
spark = SparkSession.builder \
    .appName("Verifica√ß√£o de Dados") \
    .getOrCreate()

# Ler os dados Parquet
df = spark.read.parquet("s3://meu-bucket/resultados/vendas_processadas/")

# Verificar o schema
print("Schema dos dados:")
df.printSchema()

# Contar registros
total_registros = df.count()
print(f"Total de registros: {total_registros}")

# Mostrar amostra dos dados
print("Amostra dos dados:")
df.show(5)

# Verificar estat√≠sticas b√°sicas
print("Estat√≠sticas b√°sicas:")
df.describe().show()

# Verificar distribui√ß√£o por parti√ß√£o
print("Distribui√ß√£o por ano e m√™s:")
df.groupBy("ano", "mes").count().orderBy("ano", "mes").show()

# Verificar valores nulos
print("Contagem de valores nulos por coluna:")
from pyspark.sql.functions import col, count, when
df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()

# Encerrar a sess√£o Spark
spark.stop()
```

Salve este c√≥digo em um arquivo `verificar_dados.py` e execute-o no cluster EMR:

```bash
spark-submit verificar_dados.py
```


‚û°Ô∏è Execute o script e observe os resultados da verifica√ß√£o dos dados.

## Visualizando o Conte√∫do com Athena (B√¥nus)

O Amazon Athena √© um servi√ßo de consulta interativa que permite analisar dados diretamente no S3 usando SQL padr√£o, sem necessidade de carregar os dados em um banco de dados.

### Passo 1: Acessar o Console Athena

1. Fa√ßa login no [Console AWS](https://console.aws.amazon.com/)
2. Navegue at√© o servi√ßo Athena


‚û°Ô∏è Na barra de pesquisa superior, digite "Athena" e selecione o servi√ßo "Athena" nos resultados.

### Passo 2: Configurar o Local de Resultados

Antes de usar o Athena, voc√™ precisa configurar um local no S3 para armazenar os resultados das consultas:

1. Clique em "Configura√ß√µes"
2. Em "Local dos resultados de consulta", insira um caminho S3, por exemplo:
   ```
   s3://meu-bucket/athena-results/
   ```
3. Clique em "Salvar"


‚û°Ô∏è Configure o local dos resultados de consulta no S3.

### Passo 3: Criar um Banco de Dados

1. No painel "Editor de consultas", clique no menu suspenso "Database" (Banco de dados)
2. Selecione "Create database" (Criar banco de dados)
3. Digite um nome para o banco de dados, por exemplo: `dados_processados`
4. Clique em "Create database" (Criar banco de dados)


‚û°Ô∏è Crie um novo banco de dados no Athena.

### Passo 4: Criar uma Tabela

Voc√™ pode criar uma tabela que aponta para seus dados Parquet no S3:

```sql
CREATE EXTERNAL TABLE vendas_processadas (
  id STRING,
  produto STRING,
  categoria STRING,
  valor DOUBLE,
  valor_com_imposto DOUBLE,
  data_formatada DATE,
  cliente STRING
)
PARTITIONED BY (ano INT, mes INT)
STORED AS PARQUET
LOCATION 's3://meu-bucket/resultados/vendas_processadas/';
```


‚û°Ô∏è Execute o comando SQL para criar a tabela externa.

### Passo 5: Carregar Parti√ß√µes

Para que o Athena reconhe√ßa as parti√ß√µes existentes:

```sql
MSCK REPAIR TABLE vendas_processadas;
```

Alternativamente, voc√™ pode adicionar parti√ß√µes manualmente:

```sql
ALTER TABLE vendas_processadas ADD
PARTITION (ano=2022, mes=1) LOCATION 's3://meu-bucket/resultados/vendas_processadas/ano=2022/mes=1/'
PARTITION (ano=2022, mes=2) LOCATION 's3://meu-bucket/resultados/vendas_processadas/ano=2022/mes=2/'
PARTITION (ano=2023, mes=1) LOCATION 's3://meu-bucket/resultados/vendas_processadas/ano=2023/mes=1/';
```

### Passo 6: Consultar os Dados

Agora voc√™ pode executar consultas SQL para analisar seus dados:

```sql
-- Contar registros
SELECT COUNT(*) AS total_registros FROM vendas_processadas;

-- Verificar distribui√ß√£o por parti√ß√£o
SELECT ano, mes, COUNT(*) AS contagem
FROM vendas_processadas
GROUP BY ano, mes
ORDER BY ano, mes;

-- Calcular estat√≠sticas por categoria
SELECT 
  categoria,
  COUNT(*) AS contagem,
  AVG(valor) AS valor_medio,
  MIN(valor) AS valor_minimo,
  MAX(valor) AS valor_maximo,
  SUM(valor) AS valor_total
FROM vendas_processadas
GROUP BY categoria
ORDER BY valor_total DESC;

-- Filtrar dados espec√≠ficos
SELECT *
FROM vendas_processadas
WHERE ano = 2023 AND mes = 1 AND valor > 1000
LIMIT 10;
```


‚û°Ô∏è Execute uma consulta SQL e observe os resultados.

### Passo 7: Exportar Resultados

Voc√™ pode baixar os resultados das consultas em v√°rios formatos:

1. Ap√≥s executar uma consulta, clique em "Download results" (Baixar resultados)
2. Selecione o formato desejado (CSV, JSON, etc.)

‚û°Ô∏è Clique no bot√£o de download e selecione o formato desejado.

## Verificando a Integridade dos Dados

Al√©m de visualizar os dados, √© importante verificar sua integridade:

### 1. Verificar Contagem de Registros

Compare a contagem de registros de entrada e sa√≠da:

```python
# Em PySpark
entrada_count = spark.read.csv("s3://meu-bucket/dados/vendas.csv", header=True).count()
saida_count = spark.read.parquet("s3://meu-bucket/resultados/vendas_processadas/").count()

print(f"Registros de entrada: {entrada_count}")
print(f"Registros de sa√≠da: {saida_count}")
print(f"Diferen√ßa: {abs(entrada_count - saida_count)}")
```

### 2. Verificar Consist√™ncia dos Dados

Verifique se as transforma√ß√µes foram aplicadas corretamente:

```python
# Em PySpark
# Ler dados originais
df_original = spark.read.csv("s3://meu-bucket/dados/vendas.csv", header=True)

# Ler dados processados
df_processado = spark.read.parquet("s3://meu-bucket/resultados/vendas_processadas/")

# Verificar se valor_com_imposto = valor * 1.1
from pyspark.sql.functions import col, round
df_check = df_processado.withColumn(
    "check_imposto", 
    round(col("valor") * 1.1, 2) == round(col("valor_com_imposto"), 2)
)

# Contar registros inconsistentes
inconsistentes = df_check.filter(~col("check_imposto")).count()
print(f"Registros com c√°lculo de imposto inconsistente: {inconsistentes}")
```

### 3. Verificar Particionamento

Verifique se o particionamento foi aplicado corretamente:

```python
# Em PySpark
from pyspark.sql.functions import year, month, to_date

# Verificar particionamento por ano e m√™s
df_original = spark.read.csv("s3://meu-bucket/dados/vendas.csv", header=True)
df_original = df_original.withColumn("data_formatada", to_date(col("data")))
df_original = df_original.withColumn("ano_calculado", year(col("data_formatada")))
df_original = df_original.withColumn("mes_calculado", month(col("data_formatada")))

df_processado = spark.read.parquet("s3://meu-bucket/resultados/vendas_processadas/")

# Verificar se os valores de parti√ß√£o correspondem aos valores calculados
df_check = df_processado.withColumn(
    "check_ano", 
    col("ano") == col("ano_calculado")
).withColumn(
    "check_mes",
    col("mes") == col("mes_calculado")
)

# Contar registros com particionamento inconsistente
inconsistentes_ano = df_check.filter(~col("check_ano")).count()
inconsistentes_mes = df_check.filter(~col("check_mes")).count()

print(f"Registros com ano inconsistente: {inconsistentes_ano}")
print(f"Registros com m√™s inconsistente: {inconsistentes_mes}")
```

## Boas Pr√°ticas para Verifica√ß√£o de Dados üî•

1. **Valida√ß√£o Autom√°tica**: Crie scripts de valida√ß√£o que podem ser executados automaticamente ap√≥s o processamento.

2. **Testes de Integridade**: Verifique se todos os registros foram processados e se n√£o houve perda de dados.

3. **Verifica√ß√£o de Schema**: Confirme se o schema dos dados processados est√° conforme esperado.

4. **Valida√ß√£o de Regras de Neg√≥cio**: Verifique se as transforma√ß√µes aplicadas est√£o de acordo com as regras de neg√≥cio.

5. **Monitoramento de Qualidade**: Implemente m√©tricas de qualidade de dados (completude, precis√£o, consist√™ncia).

6. **Documenta√ß√£o**: Mantenha documenta√ß√£o atualizada sobre a estrutura e o significado dos dados processados.

7. **Versionamento**: Considere versionar seus dados processados para facilitar a rastreabilidade.

8. **Backup**: Mantenha backups dos dados originais antes de aplicar transforma√ß√µes irrevers√≠veis.

## Ferramentas Adicionais para An√°lise de Dados

Al√©m do Athena, existem outras ferramentas que podem ser √∫teis para verificar e analisar dados no S3:

### 1. AWS Glue Data Catalog

O AWS Glue pode rastrear automaticamente seus dados no S3 e criar um cat√°logo de metadados:

1. No console AWS, navegue at√© o servi√ßo Glue
2. Crie um crawler para rastrear seus dados no S3
3. Execute o crawler para popular o cat√°logo de dados
4. Use o cat√°logo com Athena, Redshift Spectrum ou EMR


‚û°Ô∏è Configure um crawler para rastrear seus dados no S3.

### 2. Amazon QuickSight

O QuickSight √© um servi√ßo de visualiza√ß√£o de dados que pode se conectar diretamente ao Athena:

1. No console AWS, navegue at√© o servi√ßo QuickSight
2. Crie uma nova an√°lise
3. Selecione Athena como fonte de dados
4. Escolha o banco de dados e a tabela criados anteriormente
5. Crie visualiza√ß√µes interativas dos seus dados

‚û°Ô∏è Crie uma visualiza√ß√£o dos seus dados processados.

### 3. Jupyter Notebooks no EMR

Voc√™ pode usar Jupyter Notebooks no EMR para an√°lise interativa:

1. Ao criar o cluster EMR, selecione a aplica√ß√£o "Jupyter" ou "JupyterHub"
2. Conecte-se √† interface web do Jupyter
3. Crie um novo notebook com kernel PySpark
4. Use o c√≥digo PySpark mostrado anteriormente para analisar seus dados


‚û°Ô∏è Use o Jupyter Notebook para an√°lise interativa dos dados.

## Solu√ß√£o de Problemas Comuns

### 1. Arquivos Parquet N√£o Podem Ser Lidos

**Sintomas**: Erro ao tentar ler arquivos Parquet

**Poss√≠veis causas e solu√ß√µes**:
- **Incompatibilidade de vers√£o**: Verifique se a vers√£o do Parquet √© compat√≠vel com a ferramenta de leitura
- **Arquivos corrompidos**: Verifique se o job Spark foi conclu√≠do com sucesso
- **Permiss√µes insuficientes**: Verifique as permiss√µes IAM para acessar os objetos no S3

### 2. Parti√ß√µes N√£o Aparecem no Athena

**Sintomas**: Algumas parti√ß√µes n√£o aparecem nas consultas do Athena

**Poss√≠veis causas e solu√ß√µes**:
- **Parti√ß√µes n√£o registradas**: Execute `MSCK REPAIR TABLE` para detectar novas parti√ß√µes
- **Formato de parti√ß√£o inconsistente**: Verifique se todas as parti√ß√µes seguem o mesmo padr√£o
- **Problemas de permiss√£o**: Verifique se o Athena tem permiss√£o para acessar todas as parti√ß√µes

### 3. Contagem de Registros Inconsistente

**Sintomas**: A contagem de registros de sa√≠da n√£o corresponde √† contagem de entrada

**Poss√≠veis causas e solu√ß√µes**:
- **Filtros aplicados**: Verifique se algum filtro foi aplicado durante o processamento
- **Duplicatas**: Verifique se h√° registros duplicados nos dados de entrada ou sa√≠da
- **Erros durante o processamento**: Verifique os logs do Spark para erros

## Conclus√£o

Neste cap√≠tulo, voc√™ aprendeu como acessar e verificar arquivos Parquet e outros formatos no S3, navegar pelo console da AWS e utilizar ferramentas como o Amazon Athena para consultar e validar os dados processados. A verifica√ß√£o dos resultados √© uma etapa crucial no pipeline de processamento de dados, garantindo a qualidade e a integridade dos dados processados. No pr√≥ximo cap√≠tulo, concluiremos a apostila com um resumo dos aprendizados e refer√™ncias √∫teis para aprofundamento.

---

üîç **Pr√≥ximo Cap√≠tulo**: [Conclus√£o e Refer√™ncias](10_conclusao_referencias.md)
